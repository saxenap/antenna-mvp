{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Beautiful is better than ugly.  \n",
    "Explicit is better than implicit.  \n",
    "Simple is better than complex.  \n",
    "Complex is better than complicated.  \n",
    "Flat is better than nested.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ruleset\n",
    "rules = pd.read_csv(config.inputs['rules']['fullpath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('^(Lim Commercials 3mo|CBS - Fake Cancel|CBS All Access|Commercial Free 1 '\n",
      " 'Week|CBS - Fake New|CBS - Fake Cancel|Cancellation Confirmation CBS|CBS - '\n",
      " 'Fake Cancel Passive Churn).*$')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "For now, exclude rows that are - \n",
    "1. Missing text_match entry, and\n",
    "2. Missing text_exclude\n",
    "\"\"\"\n",
    "match_rules = rules[(rules['text_match'].notna()) & (rules['text_exclude'].notna() == False)]\n",
    "\n",
    "# Initialize the regex builder\n",
    "builder = RulesRegexBuilder(GroupedRegexConcatenation('|'))\n",
    "\n",
    "\"\"\"\n",
    "Requirement\n",
    "Add the requirements to the regex builder\n",
    "1. S (start): if the pattern in ​text_match​ \n",
    "column is a prefix of the description string, \n",
    "a match is found for the corresponding service \n",
    "(both ID and name are included in the table)\n",
    "2. A (anywhere): similar to above, except \n",
    "that the pattern doesn’t have to be in the \n",
    "beginning of the description\n",
    "3. R (regular expression): use ​text_match​ \n",
    "as a regular regular expression\n",
    "\"\"\"\n",
    "\n",
    "builder.appendDecorator(RegexDecorator('M', '(', ')'))\n",
    "builder.appendDecorator(RegexDecorator('S', '^(', ').*$'))\n",
    "builder.appendDecorator(RegexDecorator('A', '^.*(', ').*$'))\n",
    "\n",
    "\"\"\"\n",
    "Finally, we build the dictionary of regexes:\n",
    "It looks like {regex_string_to_match: service_id} for\n",
    "easier mapping with the data later.\n",
    "eg. {^.*(STARZ|Starz|STARZ|STARZ).*$': 8, ...}\n",
    "\"\"\"\n",
    "\n",
    "regexes_dict = builder.build(match_rules)\n",
    "output(random.choice(list(regexes_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'^.*(cancelled|cancel).*$': 'cancellation',\n",
      " '^.*(coming|back|signup|signing|joining|welcome).*$': 'signup'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build the dictionary for mapping signals -\n",
    "The mapping dictionay of signal keywords \n",
    "can be configured in ROOT_DIR/config.py\n",
    "\"\"\"\n",
    "\n",
    "signals = {'^.*(' + ('|'.join(v)) + ').*$' : k for k, v in config.inputs['signals'].items()}\n",
    "\n",
    "output(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBuild the dictionary for mapping service \\nnames to service ids\\n'"
      ]
     },
     "execution_count": 1070,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build the dictionary for mapping service \n",
    "names to service ids\n",
    "\"\"\"\n",
    "\n",
    "# services = rules[rules['service_id'].isin([1, 3, 8, 12, 39])][['service_id', 'service_name']]\n",
    "# services.drop_duplicates(subset = ['service_id'], inplace = True)\n",
    "# services.set_index('service_id', inplace=True)\n",
    "# doutput(services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(config.inputs['data']['fullpath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Requirement\n",
    "There 3 types of statuses:\n",
    "1. N (new): this transaction is new\n",
    "2. U (update): this transaction is \n",
    "updated; discard the old one\n",
    "3. D (delete): remove this transaction\n",
    "\"\"\"\n",
    "\n",
    "# Remove transactions with status 'D' as per requirements\n",
    "data = data[data['status'] != 'D']\n",
    "\n",
    "# Remove transactions that have an older entry\n",
    "data = data.sort_values('last_updated').drop_duplicates('item_id',keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the description column \n",
    "data['service_id'] = data[['description']]\n",
    "\n",
    "# Replace the description in the new service_id\n",
    "# column with the matching service_id from\n",
    "# regexes_dict that we created above\n",
    "data['service_id'] = data[['service_id']].replace({'service_id':regexes_dict}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Requirement\n",
    "To simplify the project, you only need to \n",
    "consider the following services, although\n",
    "the data may include many others.\n",
    "● Netflix -> 3\n",
    "● Hulu -> 1\n",
    "● CBS All Access  -> 39\n",
    "● Starz -> 8\n",
    "● Showtime -> 12\n",
    "\"\"\"\n",
    "\n",
    "data.drop(data[data['service_id'].isin([1, 3, 8, 12, 39]) == False].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the service names corresponding \n",
    "# to the service id\n",
    "data['service_name'] = data['service_id'].map(rules.set_index('service_id')['service_name'].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Requirement\n",
    "You still need to figure out what kind \n",
    "of action (signup versus cancellation) \n",
    "the remaining transactions are about.\n",
    "If there are trial signup and cancellation, \n",
    "please make your own judgment as to how \n",
    "to treat them.\n",
    "\"\"\"\n",
    "data['signal_type'] = data[['description']]\n",
    "data['signal_type'] = data[['signal_type']].replace({'signal_type': signals}, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataset locally\n",
    "save_file(config.outputs['local']['fullpath'], data.to_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⭣ This is the local link to the processed file: ⭣**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/jupyter/data/processed_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Output Local URL\n",
    "output(config.outputs['local']['fullpath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save a copy of the processed dataset \n",
    "to Google. Use IAM roles for authentication.\n",
    "\n",
    "Requirement\n",
    "Output data is accessible from a common \n",
    "cloud storage (i.e. AWS S3 or Google Storage)\n",
    "\"\"\"\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(config.outputs['cloud']['bucket_name'])\n",
    "blob = bucket.blob(config.outputs['local']['filename'])\n",
    "blob.upload_from_filename(config.outputs['local']['fullpath'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⭣ This is the Cloud URL to the processed file: ⭣**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'https://storage.googleapis.com/antenna-task/processed_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Output Cloud URL\n",
    "output(blob.public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.to_string())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
