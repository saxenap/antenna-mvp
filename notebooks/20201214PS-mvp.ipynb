{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Beautiful is better than ugly.  \n",
    "Explicit is better than implicit.  \n",
    "Simple is better than complex.  \n",
    "Complex is better than complicated.  \n",
    "Flat is better than nested.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "sys.path.append('../')\n",
    "import logging\n",
    "import random\n",
    "import numpy as np, pandas as pd\n",
    "import config\n",
    "from utilities import *\n",
    "from _rulesbuilding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ruleset\n",
    "rules = pd.read_csv(config.inputs['rules']['fullpath'])\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('^(?=(.*).*)(?!.*(a^).*).*$|^(?=(.*).*)(?!.*(a^).*).*$|^(?=(Lim Commercials '\n",
      " '3mo).*)(?!.*(a^).*).*$|^(?=(CBS - Fake Cancel).*)(?!.*(a^).*).*$|^(?=(CBS '\n",
      " 'All Access).*)(?!.*(a^).*).*$|^(?=(Commercial Free 1 '\n",
      " 'Week).*)(?!.*(a^).*).*$|^(?=(CBS - Fake New).*)(?!.*(a^).*).*$|^(?=.*(All '\n",
      " 'Access).*)(?!.*(a^).*).*$|^(?=(CBS - Fake '\n",
      " 'Cancel).*)(?!.*(a^).*).*$|^(?=(Cancellation Confirmation '\n",
      " 'CBS).*)(?!.*(a^).*).*$|^(?=(CBS - Fake Cancel Passive '\n",
      " 'Churn).*)(?!.*(a^).*).*$|^(?=.*(CBS).*)(?!.*(a^).*).*$',\n",
      " 39)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Requirement\n",
    "Add the requirements to the regex builder\n",
    "1. S (start): if the pattern in ​text_match​ \n",
    "column is a prefix of the description string, \n",
    "a match is found for the corresponding service \n",
    "(both ID and name are included in the table)\n",
    "2. A (anywhere): similar to above, except \n",
    "that the pattern doesn’t have to be in the \n",
    "beginning of the description\n",
    "3. R (regular expression): use ​text_match​ \n",
    "as a regular regular expression\n",
    "\"\"\"\n",
    "\n",
    "_rules = rules\n",
    "_rules = _rules[(_rules['text_match'].notna() == False) | (_rules['text_exclude'].notna() == False)]\n",
    "_rules['text_match'].fillna('.*', inplace = True)\n",
    "_rules['text_exclude'].fillna('a^', inplace = True)\n",
    "\n",
    "_rules['text_exclude'] = '(?!.*(' + _rules['text_exclude'] + ').*).*$'\n",
    "\n",
    "prefix = np.where(_rules['matching'] == 'A', '^(?=.*(', '^(?=(') \n",
    "_rules['text_match'] = prefix + _rules['text_match'] + ').*)' + _rules['text_exclude']\n",
    "\n",
    "_rules = _rules.groupby('service_id')['text_match'].apply(\n",
    "            lambda t: '|'.join([str(i) for i in t])\n",
    "        ).astype(str)\n",
    "\n",
    "regexes_dict = {}\n",
    "inv_map = {v: k for k, v in _rules.items()}\n",
    "regexes_dict = {**regexes_dict, **inv_map}\n",
    "\n",
    "output(random.choice(list(regexes_dict.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'^.*(cancelled|cancel).*$': 'cancellation',\n",
      " '^.*(coming|back|signup|signing|joining|welcome).*$': 'signup'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build the dictionary to map signals from\n",
    "description text -\n",
    "The mapping dictionay of signal keywords \n",
    "can be configured in ROOT_DIR/config.py\n",
    "\"\"\"\n",
    "\n",
    "signals = {'^.*(' + ('|'.join(v)) + ').*$' : k for k, v in config.inputs['signals'].items()}\n",
    "\n",
    "output(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Hulu', 3: 'Netflix', 8: 'Starz', 12: 'Showtime', 39: 'CBS All Access'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build the dictionary to map signals from\n",
    "description text -\n",
    "The mapping dictionay of signal keywords \n",
    "can be configured in ROOT_DIR/config.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "aservices = rules\n",
    "aservices = aservices[aservices['service_name'].isin(config.inputs['allowed_services'])]\n",
    "aservices = dict(zip(aservices['service_id'], aservices['service_name']))\n",
    "\n",
    "output(aservices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing ##     \n",
    "\n",
    "> Args:  \n",
    ">  1. `regexes_dict (dict of str: int): Defines the regexes that map to service ids`\n",
    "\n",
    ">  2. `signals (dict of str: int): Defines the regexes that map to signals from the description`\n",
    "\n",
    ">  3. `aservices (dict of int: str): Defines the service ids that map to allowed service names`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(config.inputs['data']['fullpath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Requirement\n",
    "There 3 types of statuses:\n",
    "1. N (new): this transaction is new\n",
    "2. U (update): this transaction is \n",
    "updated; discard the old one\n",
    "3. D (delete): remove this transaction\n",
    "\"\"\"\n",
    "\n",
    "# Remove transactions with status 'D' as per requirements\n",
    "data = data[data['status'] != 'D']\n",
    "\n",
    "# Remove transactions that have an older entry\n",
    "data = data.sort_values('last_updated').drop_duplicates('item_id',keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the description column \n",
    "data['service_id'] = data[['description']]\n",
    "\n",
    "# Replace the description in the new service_id\n",
    "# column with the matching service_id from\n",
    "# regexes_dict that we created above\n",
    "data['service_id'] = data[['service_id']].replace({'service_id':regexes_dict}, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       item_id     buyer_id  order_date      merchant_id  merchant_name    status    last_updated      description                                                                                    service_id\n",
      "--  ----------  -----------  ------------  -------------  ---------------  --------  ----------------  -------------------------------------------------------------------------------------------  ------------\n",
      " 1  1191248937  -1039890773  2016-02-19                6  Netflix          N         2016-02-20 11:18  we've cancelled your Netflix Account. This change will be effective Sunday, March 20, 2016.             1\n",
      " 2  1221099193   -751620046  2016-02-27                6  Netflix          N         2016-02-28 22:26  Thanks for joining Netflix!                                                                             1\n",
      " 3  1294618642    -78172962  2016-03-18                6  Netflix          N         2016-03-18 16:18  we've cancelled your Netflix Account. This change will be effective Monday, April 11, 2016.             1\n",
      " 5  1537007003   1799691237  2016-06-28                6  Netflix          N         2016-06-28 19:49  Thanks for joining Netflix!                                                                             1\n",
      " 6  1561855490       455386  2016-07-08              896  Spotify          N         2016-07-08 20:57  Spotify Premium                                                                                         1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Requirement\n",
    "To simplify the project, you only need to \n",
    "consider the following services, although\n",
    "the data may include many others.\n",
    "● Netflix -> 3\n",
    "● Hulu -> 1\n",
    "● CBS All Access  -> 39\n",
    "● Starz -> 8\n",
    "● Showtime -> 12\n",
    "\"\"\"\n",
    "data = data[data['service_id'].isin(list(aservices.keys()))]\n",
    "\n",
    "doutput(data.head())\n",
    "\n",
    "\n",
    "\n",
    "# Add the service names corresponding \n",
    "# to the service id\n",
    "\n",
    "# Copy the description column \n",
    "\n",
    "data['service_name'] = data[['service_id']]\n",
    "data['service_name'] = data[['service_name']].replace({'service_name': aservices})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Requirement\n",
    "You still need to figure out what kind \n",
    "of action (signup versus cancellation) \n",
    "the remaining transactions are about.\n",
    "If there are trial signup and cancellation, \n",
    "please make your own judgment as to how \n",
    "to treat them.\n",
    "\"\"\"\n",
    "data['signal_type'] = data[['description']]\n",
    "data['signal_type'] = data[['signal_type']].replace({'signal_type': signals}, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataset locally\n",
    "save_file(config.outputs['local']['fullpath'], data.to_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⭣ This is the local link to the processed file: ⭣**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/jupyter/data/processed_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Output Local URL\n",
    "output(config.outputs['local']['fullpath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save a copy of the processed dataset \n",
    "to Google. Use IAM roles for authentication.\n",
    "\n",
    "Requirement\n",
    "Output data is accessible from a common \n",
    "cloud storage (i.e. AWS S3 or Google Storage)\n",
    "\"\"\"\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(config.outputs['cloud']['bucket_name'])\n",
    "blob = bucket.blob(config.outputs['local']['filename'])\n",
    "blob.upload_from_filename(config.outputs['local']['fullpath'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⭣ This is the Cloud URL to the processed file: ⭣**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'https://storage.googleapis.com/antenna-task/processed_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Output Cloud URL\n",
    "output(blob.public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.to_string())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
